model_name_url,model_name_string,organization_and_url,publication_date,institution_type,country,organization_info,research_field,model_size_string,model_size_int,model_description,dataset,data_type,risks_and_limitations,risk_types,risk_info,license,paper_name_url
[GPT-2](https://github.com/openai/gpt-2) üìö,GPT-2,[OpenAI Inc.](https://openai.com/),2/24/2019,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Natural Language Processing",1.5B,1.5,"GPT-2 is a large language model developed by OpenAI, being able to generate human-like text from minimal prompts. The model is a decoder-only transformer pretrained on a large corpus of English data in a self-supervised fashion, meaning it learned from the raw texts without any human labels. It was trained on a dataset of over 8 million web pages and 7,000 fiction books from various genres. The resulting dataset (called [WebText](https://github.com/openai/gpt-2/blob/master/domains.txt)) weighs 40GB of text but has not been publicly released.

GPT-2 has had a significant impact on the field. The open-sourcing of the models (from [124M](https://huggingface.co/gpt2) to the [1.5B](https://huggingface.co/gpt2-xl) model) allowed for the creation of many fine-tuned versions and services based on this technology. The model was released in November 2019 and was followed by the 175-billion-parameter GPT-3 in 2020.",[WebText](https://github.com/openai/gpt-2/blob/master/domains.txt),Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","According to [GPT-2 model card](https://github.com/openai/gpt-2/blob/master/model_card.md#out-of-scope-use-cases), large-scale language models do not distinguish fact from fiction. Hence, their use in contexts that require the generated text to be true is not supported. Additionally, language models like GPT-2 [reflect the biases inherent to the systems they were trained on](https://huggingface.co/gpt2#intended-uses--limitations). The use of GPT-2 should be approached with caution around use cases that are sensitive to biases around human attributes.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",MIT License,[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
[GPT-3](https://arxiv.org/abs/2005.14165) üìö,GPT-3,[OpenAI Inc.](https://openai.com/),5/28/2020,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Natural Language Processing",175B,175,"GPT-3 is a transformer-based autoregressive language model with 175 billion parameters that achieves high performance, without any gradient updating or fine-tuning, on a wide range of NLP tasks (in a [zero-shot](https://en.wikipedia.org/wiki/Zero-shot_learning) or few-shot fashion), including translation, Q&A, word unscrambling, execute 3-digit arithmetic, text classification, sentiment analysis, translation, and many others.

The capabilities of GPT-3 have also been used for various other applications, such as [code generation](https://openai.com/blog/openai-codex/) and [chat applications](https://chat.openai.com/) being the foundation that powers many modern AI applications. The model was released in May 2020 and was followed by the 4th iteration (GPT-4) in March 2023.

GPT-3 has been quoted by numerous [researchers and philosophers](https://dailynous.com/2020/07/30/philosophers-gpt-3/), such as David Chalmers, who describes it as ""_one of the most interesting and important systems ever produced_"".","570 GB of text-format data from CommonCrawl, WebText2, Books1, Books2, and Wikipedia",Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","In ""[Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)"", the authors warn about the possible negative impacts that GPT-3 technology may cause, such as algorithmic discrimination, automation of disinformation processes, and others. To read the limitations report of GPT-3, visit OpenAI's [model card](https://github.com/openai/gpt-3/blob/master/model-card.md#limitations) page.  
  
- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.  
  
- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is, text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.  
  
- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.  
  
- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",Proprietary License,[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
[GPT-4](https://arxiv.org/abs/2303.08774) üìöüñºÔ∏è,GPT-4,[OpenAI Inc.](https://openai.com/),3/15/2023,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Reinforcement Learning, Computer Vision, Natural Language Processing",Not specified,NaN,"[GPT-4](https://arxiv.org/abs/2303.08774) is a [generative pre-trained transformer](https://paperswithcode.com/method/gpt) model, and sucessor of the GPT-3(3.5) series. Besides being capable of dealing with several NLP tasks,  GPT-4 is also a text-to-image model, making it different from its predecessors (i.e., multimodal). Also, unlike its predecessors, the GPT-4 technology already comes tuned by Reinforcement Learning from Human Feedback ([RLHF](https://huggingface.co/blog/rlhf)), like ChatGPT. 
 
The information provided in the documents [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) and [GPT-4 System Card](https://cdn.openai.com/papers/gpt-4-system-card.pdf) that accompanied the model release is limited. In these documents, no information is found about the model's architecture, size, hardware, training protocol, datasets used, or any information that can be used to replicate the technology. According to its developers, this lack of disclosure is due to the competitive landscape and security implications related to LLMs.
 
Although less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance in several professional and academic benchmarks. As demonstrated by the ChatGPT technology, the post-training alignment process (RLHF) results in performance improvements on measures of factuality and adherence to the desired behavior.",Not specified,"Text, Image",Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Malware Development, Environmental Impacts, Technological Unemployment, Intelectual Fraud","[OpenAI](https://openai.com/research/gpt-4) does not provide much information about how the [GPT-4](https://arxiv.org/abs/2303.08774) technology was developed. However, the review of the [System Card](https://cdn.openai.com/papers/gpt-4-system-card.pdf) document provides information about several risks associated with the model, including the possibility of performance failures due to [hallucinations](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)), toxic content generation, discrimination, the spread of misinformation and influence operations, proliferation of harmful content, cybersecurity risks, potential for unknown emergent behaviors, economic impacts, environmental impacts, unknown interactions with other systems, acceleration of the industry's competitive landscape, and user overconfidence.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üê±‚Äçüë§ Code generation tools can accelerate malware development, enabling malicious actors to launch more sophisticated and effective cyberattacks. These tools may also help lower the intellectual/technique barrier that prevents many people from participating in black hat hacking activities.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.

- üë®‚Äçüéì Generative models can automate the process of academic writing and intellectual creation. Such systems can impact how educational institutions function and how intellectual property laws are designed and implemented.",Proprietary License,[GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)
[CLIP](https://openai.com/blog/clip/) üìöüñºÔ∏è,CLIP,[OpenAI Inc.](https://openai.com/),2/26/2021,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Computer Vision, Natural Language Processing",Not specified,NaN,"[CLIP](https://openai.com/blog/clip/) (Contrastive Language-Image Pre-training) is a neural network capable of associating natural language snippets with images, learning the relationship between sequences of tokens and images. According to its [model card](https://github.com/openai/CLIP/blob/main/model-card.md#model-type), ""_the base model uses a ResNet50 with several modifications as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. There is also a variant of the model where the ResNet image encoder is replaced with a Vision Transformer_.""
  
According to its developers, one of the critical insights behind the CLIP model was to leverage natural language as a flexible prediction space, thus allowing for greater generalization and transfer. Hence, CLIP can be instructed in natural language, with a simple prompt agnostic training task, without needing specific phrases and labeled images.

The dataset used to train CLIP has 400 million records split between images and text collected from the internet. CLIP has been evaluated on 30 benchmarks, covering tasks like OCR, video recognition, geolocation, and other fine-grained object classification tasks. Like other foundation models, by not directly optimizing for a given benchmark/task, CLIP proves to be more general. CLIP is open source and can be used following the [linked tutorial](https://github.com/openai/CLIP).",Trained on publicly available image-caption data,"Text, Image",Yes,"Algorithmic Discrimination, Surveillance and Social Control, Environmental Impacts","The developers discuss two main broader impacts related to [CLIP](https://openai.com/blog/clip/) in their [article](https://arxiv.org/abs/2103.00020): biases and surveillance. Concerning biases, CLIP was trained on unfiltered and uncurated image-text pairs from the internet, which can result in the model learning many social biases. Also, CLIP  and similar models could enable bespoke, niche surveillance use cases for which no well-tailored models or datasets exist,  and could lower the skill requirements to build such applications. As shown by the authors, CLIP displays non-trivial, but not exceptional, performance on a few surveillance-relevant tasks today. The exact energy consumption and CO2 emissions for training the CLIP model by OpenAI are not publicly available. However, it‚Äôs known that training large language models (LLMs) like CLIP requires vast amounts of energy and has a significant carbon footprint.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üìπ AI technologies using computer vision, generative models, speech recognition, or predictive models, depending on their application, can pose a risk to individual notions of privacy, data protection, and civil liberties. These include applications designed for monitoring, surveillance, geolocation, spying, predictive policing, risk assessment algorithms, and sentence recommendation systems.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",MIT License,[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
[DALL-E](https://openai.com/blog/dall-e/) üìöüñºÔ∏è,DALL-E,[OpenAI Inc.](https://openai.com/),2/24/2021,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Computer Vision, Natural Language Processing",12B,12,"[DALL-E](https://openai.com/blog/dall-e/) is a text-to-image model comprised of two main components. The first one is a discrete variational autoencoder (dVAE) that compresses 256√ó256 RGB images into a 32 √ó 32 grid of image tokens with a vocabulary of size 8192. The second component is an autoregressive transformer that takes concatenated sequences of text tokens and image tokens to model the joint distribution over this joint latent space. DALL-E uses the standard causal mask for the text tokens, and sparse attention for the image tokens with either a row, column, or convolutional attention pattern, depending on the layer.

DALL-E can create plausible images for a great variety of sentences that explore the compositional structure of language. Also, given this compositional nature, DALL-E is capable of putting together concepts to describe both real and imaginary things. And just like other large foundation models, like GPT-3, DALL-E can perform several kinds of image-to-image translation tasks when prompted in the right way, extending the capability of large-scale neural networks to perform tasks in a zero/few-shot manner.

DALL-E is not an open-source model. However, there are some open-source alternatives to DALL-E, such as [DALL-E Mini](https://huggingface.co/dalle-mini/dalle-mini), available on Hugging Face. In regards to the images generated by DALL-E, [according to OpenAI](https://help.openai.com/en/articles/6425277-can-i-sell-images-i-create-with-dall-e), ""_you own the images you create with DALL-E_."" Hence, one is allowed to reprint, sell, and merchandise them.","250 million text-image pairs from Wikipedia, and a filtered subset from YFCC100M","Text, Image",No,"Algorithmic Discrimination, Environmental Impacts, Technological Unemployment","OpenAI does not outline the possible impacts regarding DALL-E. However, developers of [similar models](https://huggingface.co/dalle-mini/dalle-mini) state that surrogates DALL-E models can create or disseminate images that create hostile or alienating environments for people. This includes generating images that people can foreseeably find disturbing, distressing, offensive, or content that propagates historical or current stereotypes. At the same time, while DALL-E generations fall short of analogic images or human-generated content, the technology might still impact the value of human-generated art/content. Also, the training of surrogate models indicates that the development of DALL-E is associated with a significant carbon footprint.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092)
[Codex](https://openai.com/blog/openai-codex/) üìö,Codex,[OpenAI Inc.](https://openai.com/),7/7/2021,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Natural Language Processing",12B,12,"[Codex](https://openai.com/blog/openai-codex/) is a GPT model fine-tuned on code containing up to 12B parameters, capable of translating natural language sentences into programmatic code (e.g., Python). In an initial investigation of the GPT-3 model, it turned out that it could generate simple programs from docstrings. According to the OpenAI researchers, although rudimentary, this capability was exciting because GPT-3 was not explicitly trained for code generation, showing code generations could be a downstream application of this foundational model via fine-tuning.  
  
Codex can deal with languages like Python, JavaScript, Go, Perl, PHP, Ruby, Swift, TypeScript, and even Shell. Its data was compiled in May 2020 from 54 million GitHub repositories, totaling 159GB of text/code.
  
Codex served as the engine behind initial versions of [GitHub Copilot](https://copilot.github.com/). As of March 2023, the Codex models are deprecated and have been substituted by the newer [Chat models](https://platform.openai.com/docs/guides/gpt/chat-completions-api) from OpenAI.",159GB from public software repositories hosted on GitHub,Text,Yes,"Algorithmic Discrimination, Malware Development, Environmental Impacts, Technological Unemployment","In their [published paper](https://arxiv.org/pdf/2107.03374.pdf), the researchers responsible discuss the potential broader impacts related to the Codex technology. They show that one of the main risks to code generation models is over-reliance on the results generated, reporting that ""_Codex may suggest solutions that superficially appear correct but do not accomplish the task intended by the user_."" The authors also warn about misalignment, in which the model may have a capability but does not use it to aid the user. Other risks cited by developers include the problem of representativeness, labor displacement, economic and environmental impacts, legal implications, and its repercussions for (cyber) security, among others.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üê±‚Äçüë§ Code generation tools can accelerate malware development, enabling malicious actors to launch more sophisticated and effective cyberattacks. These tools may also help lower the intellectual/technique barrier that prevents many people from participating in black hat hacking activities.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374)
[GLIDE](https://gpt3demo.com/apps/openai-glide) üìöüñºÔ∏è,GLIDE,[OpenAI Inc.](https://openai.com/),12/20/2021,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Computer Vision, Natural Language Processing",3.5B,3.5,"[GLIDE](https://gpt3demo.com/apps/openai-glide) (Guided Language to Image Diffusion for Generation and Editing) is a [diffusion model](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) that generates images through natural language. GLIDE also allows editions to be made to existing images using natural language prompts. These edits include inserting new objects, adding shadows and reflections, conducting images into the painting, and so on. 
  
According to OpenAI, human evaluators preferred the output images of GLIDE (3.5 billion parameters) to those of [DALL-E](https://openai.com/blog/dall-e/) (12 billion parameters), even though GLIDE is a considerably smaller model.  
  
OpenAI released a smaller model of GLIDE (GLIDE  filtered), trained with filtered data, whose code and weights are available on the project's [GitHub](https://github.com/openai/glide-text2im). Other models are not available to the public.",Not specified,"Text, Image",Yes,"Disinformation, Algorithmic Discrimination, Environmental Impacts, Technological Unemployment","In GLIDE's  [model card](https://github.com/openai/glide-text2im/blob/main/model-card.md#limitations), authors cite some of the major limitations of this technology. For example, (1) it produces different outputs when asked to generate toys for boys and toys for girls, (2) it gravitates toward generating images of churches when asked to generate ""_a religious place_"", and (3) it may have a greater propensity for generating hate symbols other than swastikas and confederate flags.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is, text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner. 

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis. 

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",MIT License*,[GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741v1)
[DALL-E 2](https://openai.com/dall-e-2/) üìöüñºÔ∏è,DALL-E 2,[OpenAI Inc.](https://openai.com/),4/13/2022,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Computer Vision, Natural Language Processing",3.5B,3.5,"[DALL-E 2](https://openai.com/dall-e-2) is a text-to-image model composed of two main parts: an encoder that generates a CLIP image embedding given a text caption and a decoder that generates an image conditioned on the image embedding. The result of this combination is DALL-E 2, a multimodal model that can generate photo-realistic images from simple natural language prompts. DALL-E 2 was trained on pairs of images and their corresponding captions drawn from a combination of publicly available sources and sources licensed by OpenAI.

In addition to generating images based on text description prompts, DALL-E 2 can modify existing images as prompted using a text description. It can also take an existing image as an input and be prompted to produce a creative variation on it. 

Currently, DALL-E 2 can be accessed via API in a restricted and controlled way by OpenAI.",Encoder dataset: DALL-E and CLIP dataset (approximately 650M images). Decoder dataset: DALL-E dataset (approximately 250M images),"Text, Image",Yes,"Disinformation, Algorithmic Discrimination, Environmental Impacts, Technological Unemployment","On DALL-E 2 [model card](https://github.com/openai/dalle-2-preview/blob/main/system-card.md#dalle-2-preview---risks-and-limitations), the researchers describe the risks and limitations of the model, such as the presence of explicit content in its dataset, bias, and the potentials for harassment, intimidation, exploitation, and misinformation that could come from the misuse of this technology. The model card also outlines all measures OpenAI has taken to minimize these possible cases of misuse and unwanted side effects.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/abs/2204.06125)
[DALL-E 3](https://openai.com/dall-e-3/) üìöüñºÔ∏è,DALL-E 3,[OpenAI Inc.](https://openai.com/),9/20/2023,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Computer Vision, Natural Language Processing",Not specified,NaN,"[DALL-E 3](https://openai.com/dall-e-3) is the successor of DALL-E 2, which, according to OpenAI's release, can understand significantly more nuance and detail instructions than previous versions, diminishing the need for prompt engineering in its use.

Not much is known about the architecture, training protocol, or dataset involved in the development of this technology. However, one can speculate that the system is an improved version of the encoder-decoder type model used in DALL-E 2, but now based on the GPT-4 model and trained with an improved dataset.

Currently, DALL-3 powers Bing Chat assistant and is built natively on ChatGPT, following similar release protocols to earlier versions of the technology (i.e., restricted access via the OpenAI API).",Not specified,"Text, Image",No,"Disinformation, Algorithmic Discrimination, Environmental Impacts, Technological Unemployment","According to [OpenAI's release blog post](https://openai.com/dall-e-3), DALL-E 3 has mitigations to decline requests that ask for a public figure by name and has improved safety performance in risk areas like generation of public figures and harmful biases related to visual over/under-representation. Also, OpenAI states to be developing an internal tool that can help one identify whether or not an image was generated by DALL-E 3. DALL-E 3 can also decline requests that ask for an image in the style of a living artist.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[DALL-E 3: OpenAI's Blog](https://openai.com/dall-e-3)
[ChatGPT](https://openai.com/blog/chatgpt/) üìö,ChatGPT,[OpenAI Inc.](https://openai.com/),11/30/2022,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Reinforcement Learning, Natural Language Processing",175B,175,"[ChatGPT](https://openai.com/blog/chatgpt/) is a large language model based on an improved version of GPT-3, developed in a similar way to [InstructGPT](https://arxiv.org/abs/2203.02155), which is trained to follow an instruction in a prompt.

ChatGPT was trained by OpenAI with reinforcement learning from human feedback ([RLHF](https://huggingface.co/blog/rlhf)). The basic idea behind RLHF is that a language model can be considered a policy over a vocabulary, which allows one to use RL techniques (e.g., optimizing over a reward model) for training purposes. In RLHF, this reward model is trained to be a surrogate for human evaluations. Hence, this synthetic version of human approval becomes the learning signal for the policy during training.

ChatGPT can provide answers to complex questions, [utilize plugins](https://openai.com/blog/chatgpt-plugins), and query other OpenAI models (e.g., DALL-E 3), being the core engine being the [Chat completions API](https://platform.openai.com/docs/api-reference/chat) from OpenAI. Currently, the model is available for use via the OpenAI platform ([chat.openai](https://chat.openai.com/)) and API, where users can interreact with the model using GPT-3.5 and GPT-4 as the base model.",Improved version of GPT-3 dataset + human demonstrations/evaluations,Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Malware Development, Environmental Impacts, Technological Unemployment, Intelectual Fraud","According to the ChatGPT introduction [homepage](https://openai.com/blog/chatgpt/), there are several limitations related to the ChatGPT technology. For example, ChatGPT has shortcomings concerning generating grounded information being prone to produce plausible-sounding but incorrect or nonsensical answers. The model is also capable of responding to harmful instructions or exhibiting biased behavior. Even though several [guardrails](https://openai.com/blog/new-and-improved-content-moderation-tooling) exist to protect users, jailbreaks are still possible.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üê±‚Äçüë§ Code generation tools can accelerate malware development, enabling malicious actors to launch more sophisticated and effective cyberattacks. These tools may also help lower the intellectual/technique barrier that prevents many people from participating in black hat hacking activities.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.

- üë®‚Äçüéì Generative models can automate the process of academic writing and intellectual creation. Such systems can impact how educational institutions function and how intellectual property laws are designed and implemented.",Proprietary License,[Introducing ChatGPT: OpenAI's blog](https://openai.com/blog/chatgpt)
[InstructGPT](https://arxiv.org/abs/2203.02155) üìö,InstructGPT,[OpenAI Inc.](https://openai.com/),3/4/2022,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Reinforcement Learning, Natural Language Processing",175B,175,"[InstructGPT](https://arxiv.org/abs/2203.02155) is a fined-tuned version of OpenAI's [GPT-3](https://arxiv.org/abs/2005.14165), achieved via a mix of supervised fine-tuning and reinforcement learning from human feedback. While GPT-3 was trained via causal language modeling, that is, to predict the next token in a sequence of tokens, InstructGPT was trained to model (minimize cross-entropy loss) the distribution of its fine-tuning dataset, comprised of human demonstrations of appropriate instruction-following behavior, and later to maximize the cumulative reward of a learning signal that serves as a proxy for human evaluations. 

In general, this process can be divided into three steps: (1) Prompt/compilation pairs are collected (using human contractors to create them) and used to fine-tune the base model (GPT-3). (2) After fine-tuning, the model generates several outputs per sample (prompt). Crowdsourced human evaluators then score each output. This scoring is then used to train a reward model. (3) The reward model is then used to update the fine-tuned model again via [proximal policy optimization](https://openai.com/blog/openai-baselines-ppo/).

The final result of this process is InstructGPT, which comes in three different sizes (1.3B, 6B, and 175B parameters), fine-tuned from the Babbage, Curie, and Davinci versions of GPT-3 (currently deprecated). InstructGPT represents a significant advance towards alignment when compared to other LLMs, being capable of creating fewer imitative falsehoods, making up facts less frequently, and generating more acceptable outputs. InstructGPT can be said to be a predecessor of more capable/aligned models, like [ChatGPT](https://openai.com/blog/chatgpt/).",Prompt/completion pairs submitted to the OpenAI API,Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Malware Development, Environmental Impacts, Technological Unemployment, Intelectual Fraud","The training techniques used to improve GPT-3 and develop InstructGPT have the drawback of introducing a potential alignment cost, which means that by focusing on aligning the models, especially to user tasks, the models may unintentionally perform worse on other NLP tasks. To slightly traverse this problem, OpenAI blends in a small portion of the original data used to train GPT-3, which helped InstructGPT to maintain general performance. However, InstructGPT still makes errors, like incorrectly assuming a premise is true when given instructions with a false premise, and offering multiple answers when only one clear answer is required. Also, like GPT-3, InstructGPT still produces harmful or biased outputs, makes up some facts, and produces sexual and violent content without specific provocation.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üê±‚Äçüë§ Code generation tools can accelerate malware development, enabling malicious actors to launch more sophisticated and effective cyberattacks. These tools may also help lower the intellectual/technique barrier that prevents many people from participating in black hat hacking activities.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.

- üë®‚Äçüéì Generative models can automate the process of academic writing and intellectual creation. Such systems can impact how educational institutions function and how intellectual property laws are designed and implemented.",Proprietary License,[Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/abs/2203.02155)
[Whisper](https://arxiv.org/abs/2212.04356) üì¢üìö,Whisper,[OpenAI Inc.](https://openai.com/),12/6/2022,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Natural Language Processing, Speech Recognition",1.55B,1.55,"[Whisper](https://arxiv.org/abs/2212.04356) is a general-purpose speech recognition model based on the original [encoder-decoder transformer](https://arxiv.org/abs/1706.03762) architecture trained on over 96 languages (other than English). Whisper can perform multilingual speech recognition, translation, and language identification, among other tasks. These multitask capabilities came via specification techniques (i.e., conditional training) used to enable a single input signal (and model) to generate different outputs (depending on the context of the task).

OpenAI trained several models of different sizes, from the smallest (39M parameters) to the largest (1550M), called [large-v2](https://github.com/openai/whisper/discussions/661). These models were released on different dates, the first in September 2022 and the last model (the largest) in December 2022. In their publication, researchers report (except for English speech recognition) that the performance of Whisper technology on tasks such as multilingual speech recognition, speech translation, and language identification continues to increase with model size. At the same time, increases in dataset size result in improved performance on all tasks, although significant variability in improvement rates across tasks and sizes occurs.
  
Whisper is an open-source model intended to ""_serve as a basis for building useful applications and for further research on speech processing_.""","680,000 hours of labeled audio","Text, Audio",Yes,"Disinformation, Algorithmic Discrimination, Environmental Impacts, Surveillance and Social Control, Technological Unemployment","The developers of [Whisper](https://arxiv.org/abs/2212.04356) reported in their paper a selection of limitations of their model, such as getting stuck in repeated loops, not transcribing the first or last word of an audio segment or the problem in which the model will output a transcript unrelated to the real audio. At the same time, Whisper exhibits disparate performance on different accents and dialects of particular languages, which may include a higher word error rate across speakers of different genders, races, ages, or other demographic criteria. In their [model letter](https://github.com/openai/whisper/blob/main/model-card.md), Whisper developers show concerns such as the fact that such technology may allow actors to build new methods to automate surveillance. Furthermore, Whisper models can be used to recognize specific individuals, which in turn presents security concerns related to misuse and abuse of the technology.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üìπ AI technologies using computer vision, generative models, speech recognition, or predictive models, depending on their application, can pose a risk to individual notions of privacy, data protection, and civil liberties. These include applications designed for monitoring, surveillance, geolocation, spying, predictive policing, risk assessment algorithms, and sentence recommendation systems.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",MIT License,[Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356)
[WebGPT](https://arxiv.org/abs/2112.09332) üìö,WebGPT,[OpenAI Inc.](https://openai.com/),12/17/2021,Non-profit,United States of America,"[OpenAI](https://openai.com/) is an American artificial intelligence research laboratory consisting of the non-profit OpenAI, Inc. and its for-profit subsidiary corporation OpenAI, L.P, founded in 2015 by Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, Jessica Livingston, John Schulman, Pamela Vagata, and Wojciech Zaremba. Their mission ""_is to ensure that General Artificial Intelligence benefits all of humanity_.""  
  
The organization maintains an active research and development agenda in AI Safety. In ""_[Our approach to AI safety](https://openai.com/blog/our-approach-to-ai-safety)_"", OpenAI states that it is committed to keeping Artificial Intelligence safe and broadly beneficial. ","Deep Learning, Reinforcement Learning, Natural Language Processing",175B,175,"[WebGPT](https://openai.com/research/webgpt) is a fine-tuned version of [GPT-3](https://arxiv.org/abs/2005.14165). While GPT-3 tends to hallucinate information when performing tasks requiring real-world knowledge, WebGPT was trained to search the web via a text-based web browser and generate responses via the retrieved information.
  
The model was tuned through a combination of imitation learning and behavior cloning (using human experts as a reference signal), reinforcement learning from human feedback, and rejection sampling, where a reward model trained by human feedback was used to sample the best responses generated by the model. The comparisons evaluated by WebGPT can be found in [this repository](https://huggingface.co/datasets/openai/webgpt_comparisons).
  
WebGPT was evaluated against benchmarks such as [ELI5](https://arxiv.org/abs/1907.09190) and [TruthfulQA](https://arxiv.org/abs/2109.07958), demonstrating superior performance to its foundation in matters of response preferability and factual groundness.",A collection of demonstrations and comparisons made by freelance contractors from Upwork (https://www.upwork.com) and Surge AI (https://www.surgehq.ai),Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts, Technological Unemployment","In ""[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)"", the authors argue that even if WebGPT is generally more truthful than GPT-3, the model still makes basic errors. Also, the authors point out that by providing answers with citations, WebGPT is often perceived as having an air of authority, which can obscure the fact that the model still may hallucinate. The model is also prone to reinforce the existing beliefs of users and act in a biased form in certain situations. In addition to these deployment risks, the author's approach introduces new risks at train time by giving the model access to the web.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/abs/2112.09332)
[BlenderBot 3](https://arxiv.org/abs/2208.03188) üìö,BlenderBot 3,[Meta AI](https://ai.meta.com/),8/5/2022,Public Company,United States of America,"[Meta AI](https://ai.facebook.com/) is the AI research division of Meta Platforms, Inc., known as Meta] and formerly Facebook, Inc. Meta is an American multinational technology conglomerate based in Menlo Park, California. Meta AI started as Facebook Artificial Intelligence Research (FAIR), announced in September 2013, and initially directed by [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun ""Yann LeCun"").

When it comes to matters of AI Ethics, [Meta AI expresses](https://ai.meta.com/about/) that ""_Our commitment to responsible AI is driven by the belief that everyone should have equitable access to information, services, and opportunities_"". Meta AI also claims to adhere to Meta's core principles: Privacy and security, Fairness and inclusion, Robustness and safety, Transparency and control, Accountability and governance, among other key principles.","Deep Learning, Natural Language Processing",175B,175,"[BlenderBot 3](https://about.fb.com/news/2022/08/blenderbot-ai-chatbot-improves-through-conversation/) is a 175B parameter dialogue model capable of open-domain conversation with access to the internet and long-term memory. The model was developed using [OPT-175B](https://arxiv.org/abs/2205.01068) as its foundation, being the continuation of the [BlenderBot series](https://ai.meta.com/blog/state-of-the-art-open-source-chatbot/), and approximately 58 times the size of [BlenderBot 2](https://ai.meta.com/blog/blender-bot-2-an-open-source-chatbot-that-builds-long-term-memory-and-searches-the-internet/).

The result is a conversational model that [learns from interactions and feedback](https://parl.ai/projects/fits). BlenderBot 3 is designed to improve its capabilities through natural conversations and feedback from its users, being one of the first chatbots able to build long-term memory and continuously access and search the web.
  
BlenderBot 3 comes in three sizes: 3B, 30B, and 175B parameters. While the 3B and 30B models are available in the [ParlAI model zoo](https://parl.ai/docs/zoo.html), the 175B parameter version requires registration and acceptance of the terms and conditions of the BB3-175B License Agreement.",Approximately 1.3B training tokens. A complete list of training datasets can be found in this [data card](https://github.com/facebookresearch/ParlAI/blob/main/parlai/zoo/bb3/data_card.md),Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","In their [publication](https://arxiv.org/abs/2208.03188), the authors present some of the risks and limitations associated with BlenderBot 3. It is cited that, like other large language models, [BlenderBot 3](https://geo-not-available.blenderbot.ai/) ""_is not perfect and makes several mistakes, ranging from being out of context, meaningless, incorrect, or sometimes rude/inappropriate_"". A complete list of limitations (e.g., discriminatory biases, tendency to hallucinate, etc.) can also be found in BlenderBot 3 [model card](https://github.com/facebookresearch/ParlAI/blob/main/parlai/zoo/bb3/model_card.md).

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",BB3-175B License,[BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage](https://arxiv.org/abs/2208.03188)
[OPT-175B](https://arxiv.org/abs/2205.01068) üìö,OPT-175B,[Meta AI](https://ai.meta.com/),5/2/2022,Public Company,United States of America,"[Meta AI](https://ai.facebook.com/) is the AI research division of Meta Platforms, Inc., known as Meta] and formerly Facebook, Inc. Meta is an American multinational technology conglomerate based in Menlo Park, California. Meta AI started as Facebook Artificial Intelligence Research (FAIR), announced in September 2013, and initially directed by [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun ""Yann LeCun"").  
  
When it comes to matters of AI Ethics, [Meta AI expresses](https://ai.meta.com/about/) that ""_Our commitment to responsible AI is driven by the belief that everyone should have equitable access to information, services, and opportunities_"". Meta AI also claims to adhere to Meta's core principles: Privacy and security, Fairness and inclusion, Robustness and safety, Transparency and control, Accountability and governance, among other key principles.","Deep Learning, Natural Language Processing",175B,175,"[OPT-175B](https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/) is a language model with 175 billion parameters, following a similar architectural design to that of GPT models, trained on publicly available data sets, to allow for more community engagement in understanding foundational models.

According to Meta AI, OPT-175B is comparable to GPT-3, while requiring only 1/7th of the carbon footprint to develop. Like other foundational models, OPT-175B can perform several NLP tasks out of the box in a zero/few-shot fashion.

OPT models were trained from a range between 125M to the 175B parameters model. All of them are openly released under the [OPT-175B License Agreement](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md).",Approximately 180B tokens corresponding to 800 GB of data. A complete list of datasets used is listed in [Appendix C](https://arxiv.org/abs/2205.01068),Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","According to the authors, OPT-175B suffers from the same limitations noted in other LLMs, like not working well with declarative instructions or point-blank interrogatives, being repetitive, producing factually incorrect statements, and showing a propensity to generate toxic language and reinforce harmful stereotypes.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",OPT-175B License,[OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)
[Galactica](https://galactica.org/explore/) üìö,Galactica,[Meta AI](https://ai.meta.com/),11/16/2022,Public Company,United States of America,"[Meta AI](https://ai.facebook.com/) is the AI research division of Meta Platforms, Inc., known as Meta] and formerly Facebook, Inc. Meta is an American multinational technology conglomerate based in Menlo Park, California. Meta AI started as Facebook Artificial Intelligence Research (FAIR), announced in September 2013, and initially directed by [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun ""Yann LeCun"").  
  
When it comes to matters of AI Ethics, [Meta AI expresses](https://ai.meta.com/about/) that ""_Our commitment to responsible AI is driven by the belief that everyone should have equitable access to information, services, and opportunities_"". Meta AI also claims to adhere to Meta's core principles: Privacy and security, Fairness and inclusion, Robustness and safety, Transparency and control, Accountability and governance, among other key principles.","Deep Learning, Natural Language Processing",120B,120,"[Galactica](https://github.com/paperswithcode/galai) is a large language model that can store, combine, and reason about scientific knowledge. Galactica was trained in five sizes (from 125M to 120B), using a large scientific corpus of papers, reference material, knowledge bases, and many other sources. It can perform scientific NLP tasks at a high level, like citation prediction, mathematical reasoning, molecular property prediction, protein annotation, summarization, and entity extraction.

Galactica is not instruction-tuned, which means it requires the use of prompts and special tokens (e.g., [START_REF], [END_REF]) to produce the intended behavior. These capabilities were developed via prompt pre-training, enabling the model to work out of the box for popular tasks.

Galactica is available as an open-source model and can be used through the [Galai](https://github.com/paperswithcode/galai) module (Python). Models are also available for download on [Hugging Face](https://huggingface.co/facebook/galactica-1.3b).","106 billion tokens of articles, reference materials, encyclopedias, and other scientific sources",Text,Yes,"Disinformation, Algorithmic Discrimination, Environmental Impacts, Intelectual Fraud","In their [paper](https://arxiv.org/abs/2211.09085), the authors mention that the model requires augmentation before being used in a production environment. They also note the model's propensity, although reduced when compared to other foundational models, to produce stereotypical biases. The same can be said concerning the tendency of this model to generate wrong information.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë®‚Äçüéì Generative models can automate the process of academic writing and intellectual creation. Such systems can impact how educational institutions function and how intellectual property laws are designed and implemented.",CC BY-NC-SA 4.0,[Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)
[LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) üìö,LLaMA,[Meta AI](https://ai.meta.com/),2/27/2023,Public Company,United States of America,"[Meta AI](https://ai.facebook.com/) is the AI research division of Meta Platforms, Inc., known as Meta] and formerly Facebook, Inc. Meta is an American multinational technology conglomerate based in Menlo Park, California. Meta AI started as Facebook Artificial Intelligence Research (FAIR), announced in September 2013, and initially directed by [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun ""Yann LeCun"").  
  
When it comes to matters of AI Ethics, [Meta AI expresses](https://ai.meta.com/about/) that ""_Our commitment to responsible AI is driven by the belief that everyone should have equitable access to information, services, and opportunities_"". Meta AI also claims to adhere to Meta's core principles: Privacy and security, Fairness and inclusion, Robustness and safety, Transparency and control, Accountability and governance, among other key principles.","Deep Learning, Natural Language Processing",65B,65,"[LLaMA](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) is a collection of foundation language models ranging from 7B to 65B parameters, trained on over a trillion tokens of publicly available data. According to Meta AI, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B.

LLaMA models are GPT-style autoregressive transformers trained on a substantially more significant amount of data than other language models, following the [Chinchilla scaling laws](https://arxiv.org/abs/2203.15556), i.e., a set of empirical rules that describe the optimal trade-off between model size and training data for LLMs. LLaMA models can perform several NLP tasks in a zero/few-shot fashion, like closed-book questions and answering, mathematical reasoning, reading comprehension, and code generation.

LLaMA models were released under a noncommercial license focused on research use cases. [Access to the models](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) is granted on a case-by-case basis to academic researchers; those affiliated with organizations in government, civil society, and academia; and industry research laboratories around the world.",1.4 trillion tokens drawn from publicly available data sources and text from 20 different languages,Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","According to the developers of [LLaMA](https://arxiv.org/abs/2302.13971), the risks involved with using large language models like LLaMA include the creation of toxic content with hate speech and profane language, the dissemination of misinformation due to inaccuracies of information in the training data, and a large carbon footprint related to its development.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",LLaMA 2 Community License Agreement,"[LLaMA: A foundational, 65-billion-parameter large language model](https://arxiv.org/abs/2302.13971)"
[LLaMA 2](https://arxiv.org/abs/2307.09288) üìö,LLaMA 2,[Meta AI](https://ai.meta.com/),7/18/2023,Public Company,United States of America,"[Meta AI](https://ai.facebook.com/) is the AI research division of Meta Platforms, Inc., known as Meta] and formerly Facebook, Inc. Meta is an American multinational technology conglomerate based in Menlo Park, California. Meta AI started as Facebook Artificial Intelligence Research (FAIR), announced in September 2013, and initially directed by [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun ""Yann LeCun"").  
  
When it comes to matters of AI Ethics, [Meta AI expresses](https://ai.meta.com/about/) that ""_Our commitment to responsible AI is driven by the belief that everyone should have equitable access to information, services, and opportunities_"". Meta AI also claims to adhere to Meta's core principles: Privacy and security, Fairness and inclusion, Robustness and safety, Transparency and control, Accountability and governance, among other key principles.","Deep Learning, Reinforcement Learning, Natural Language Processing",70B,70,"[Llama 2](https://arxiv.org/abs/2307.09288) is a collection of pretrained and fine-tuned large language models ranging in scale from 7B to 70B parameters. This model is an updated version of [Llama 1](https://arxiv.org/abs/2302.13971), trained on a new mix of publicly available data (increased the size of the pretraining corpus by 40%), with doubled the context length of the Llama 1, and
adopting the use of [grouped-query attention](https://arxiv.org/abs/2305.13245) (allowing faster inference). Meanwhile, the fine-tuned versions of Llama 2, called Llama 2-Chat, are optimized for dialogue use cases.

The fine-tuned versions of Llama 2 were achieved via preference modeling techniques like [Reinforcement Learning from Human Feedback](https://huggingface.co/blog/rlhf). The RLHF approaches used by the authors were variants of [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) and [Rejection Sampling](https://arxiv.org/abs/2204.05862) fine-tuning.

Llama 2 models are available under the [LLaMA 2 Community License Agreement](https://ai.meta.com/llama/license/), making Llama models available for commercial and research use.",2 trillion tokens with over 1 million human annotations,Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Malware Development, Environmental Impacts, Technological Unemployment, Intelectual Fraud","According to [Meta AI](https://llama-2.ai/llama-2-model-details/), Llama 2 is a new technology that carries risks with use. Testing conducted to date has not cover all scenarios, including uses in languages other than English. For these reasons, as with all LLMs, Llama 2‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. 

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üê±‚Äçüë§ Code generation tools can accelerate malware development, enabling malicious actors to launch more sophisticated and effective cyberattacks. These tools may also help lower the intellectual/technique barrier that prevents many people from participating in black hat hacking activities.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.

- üë®‚Äçüéì Generative models can automate the process of academic writing and intellectual creation. Such systems can impact how educational institutions function and how intellectual property laws are designed and implemented.",LLaMA 2 Community License Agreement,[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
[CICERO](https://ai.meta.com/research/cicero/) üìöüïπÔ∏è,CICERO,[Meta AI](https://ai.meta.com/),11/22/2022,Public Company,United States of America,"[Meta AI](https://ai.facebook.com/) is the AI research division of Meta Platforms, Inc., known as Meta] and formerly Facebook, Inc. Meta is an American multinational technology conglomerate based in Menlo Park, California. Meta AI started as Facebook Artificial Intelligence Research (FAIR), announced in September 2013, and initially directed by [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun ""Yann LeCun"").  
  
When it comes to matters of AI Ethics, [Meta AI expresses](https://ai.meta.com/about/) that ""_Our commitment to responsible AI is driven by the belief that everyone should have equitable access to information, services, and opportunities_"". Meta AI also claims to adhere to Meta's core principles: Privacy and security, Fairness and inclusion, Robustness and safety, Transparency and control, Accountability and governance, among other key principles.","Deep Learning, Reinforcement Learning, Natural Language Processing",2.7B,2.7,"[Cicero](https://github.com/facebookresearch/diplomacy_cicero) is an AI agent that can achieve human-level performance in [Diplomacy](https://en.wikipedia.org/wiki/Diplomacy_(game)), a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players. 

Cicero integrates several models, from language models (R2C2-based transformer encoder-decoder with 2.7B parameters) to reinforcement learning policies and value networks (transformer encoder), combining strategic reasoning and natural language processing. According to the authors, across 40 games of an anonymous online Diplomacy league, Cicero achieved more than double the average score of the human players and ranked in the top 10% of participants who played more than one game.

The source code to create Cicero, as the weights of the models involved, are released under an [MIT](https://github.com/facebookresearch/diplomacy_cicero/blob/main/LICENSE.md) and  [CC BY-NC-SA 4.0](https://github.com/facebookresearch/diplomacy_cicero/blob/main/LICENSE_FOR_MODEL_WEIGHTS.txt) licenses, respectively.",A dataset of almost 13M messages from online Diplomacy games,Text,No,"Algorithmic Discrimination, Social Engineering, Environmental Impacts","Cicero can use natural language to influence other players‚Äô decisions and actions, sometimes by lying or bluffing. These tendencies could raise ethical concerns about the honesty and transparency of Cicero‚Äôs communication and the potential of training language models that are proficient at deceiving humans. Also, Cicero can behave in a discriminatory way by favoring or avoiding certain players based on their nationality, gender, age, or personality.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",CC BY-NC-SA 4.0,[Human-level play in the game of Diplomacy by combining language models with strategic reasoning](https://www.science.org/doi/10.1126/science.ade9097)
[ESM-2](https://www.science.org/doi/abs/10.1126/science.ade2574) üß¨,ESM-2,[Meta AI](https://ai.meta.com/),3/16/2023,Public Company,United States of America,"[Meta AI](https://ai.facebook.com/) is the AI research division of Meta Platforms, Inc., known as Meta] and formerly Facebook, Inc. Meta is an American multinational technology conglomerate based in Menlo Park, California. Meta AI started as Facebook Artificial Intelligence Research (FAIR), announced in September 2013, and initially directed by [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun ""Yann LeCun"").  
  
When it comes to matters of AI Ethics, [Meta AI expresses](https://ai.meta.com/about/) that ""_Our commitment to responsible AI is driven by the belief that everyone should have equitable access to information, services, and opportunities_"". Meta AI also claims to adhere to Meta's core principles: Privacy and security, Fairness and inclusion, Robustness and safety, Transparency and control, Accountability and governance, among other key principles.","Deep Learning, Pattern Recognition, Forecasting",15B,15,"[ESM-2](https://github.com/facebookresearch/esm) is a SOTA general-purpose protein language model that can be used to predict structure, function, and other protein properties directly from individual sequences of amino acids. The ESM-2 series was trained on a range from 8M to 15B parameters, being the continuation of other models developed by Meta AI (ESMFold, ESM-1, etc.), created to tackle the [protein folding problem](https://en.wikipedia.org/wiki/Protein_folding).
  
ESM-2 was trained on an unsupervised masked language modeling task (i.e., predicting the identity of randomly selected amino acids in a protein sequence by observing their context in the rest of the series). One of the significant contributions this technology makes is in developing a prediction method that eliminates costly aspects of current state-of-the-art structure prediction methods by removing the need for multiple sequence alignment while greatly simplifying the neural architecture used for inference. This results in up to 60x speed improvement in inference, completely removing the related protein search process, which can take over 10 minutes with models such as [AlphaFold](https://www.nature.com/articles/s41586-021-03819-2) and [RosettaFold](https://www.science.org/doi/10.1126/science.abj8754).
  
All ESM models are licensed under an MIT License and available on Meta's Research [GitHub repository](https://github.com/facebookresearch/esm).",Uniref50 (UR50) dataset: a biological dataset taken from the [Uniprot database](https://www.uniprot.org/),Biological Data,No,"Biological Risks, Environmental Impacts","The ability to predict protein structures may profoundly impact various aspects of society, such as health, agriculture, biotechnology, and biosecurity. For example, it may enable the discovery of new drugs or vaccines for treating diseases, but it may also facilitate the creation of novel bioweapons or pathogens. At the same time, due to inaccuracies and hallucinations, these models may generate incorrect or harmful predictions that may cause adverse reactions or damage to biological systems.

- ‚ò£Ô∏è Models that can predict protein structures have the potential to be used to design and synthesize proteins with specific properties, including the ability to target and attack organisms or tissues, enabling the development of harmful biological agents.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",MIT License,[Evolutionary-scale prediction of atomic level protein structure with a language model](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v3)
[BLOOM](https://huggingface.co/bigscience/bloom) üìö,BLOOM,[BigScience](https://bigscience.huggingface.co/),11/9/2022,Academic/Research Institution,France,"[BigScience](https://bigscience.huggingface.co/) is an open and collaborative workshop around studying and creating very large language models, gathering more than 1000 researchers worldwide.

According to their [homepage](https://bigscience.notion.site/Introduction-5facbf41a16848d198bda853485e23a0), the BigScience project is inspired by existing partnership projects in other fields, such as [CERN](https://home.web.cern.ch/), [LIGO]( https://www.ligo.caltech.edu/), and [ITER](https://www.iter.org/), in which research collaborations are open, facilitating large-scale results.","Deep Learning, Natural Language Processing",176B,176,"BigScience Large Open-science Open-access Multilingual Language Model ([BLOOM](https://huggingface.co/bigscience/bloom)) is a transformer-based large language model created by over 1,000 AI researchers. BLOOM was trained on around 366 billion tokens from March through July 2022, and it was one of the first open alternatives to large language models like GPT-3.

BLOOM uses a decoder-only transformer model architecture modified from Megatron-LM GPT-2. It can output coherent text in 46 languages and 13 programming languages that are hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for.

BLOOM was trained in six sizes ranging from 560 million to 176 billion parameters. All models are [publicly available](https://huggingface.co/bigscience) and released under the Responsible AI License.","[ROOTS corpus](https://arxiv.org/abs/2303.03915), a dataset comprising hundreds of sources in 46 natural and 13 programming languages (366B tokens)",Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","BLOOM's [model letter](https://huggingface.co/bigscience/bloom) documents its risks and limitations. In it, the authors warn about BLOOM's tendencies to generate toxic content and untruthful information, among other risks related to using LLMs. Regarding BLOOM's environmental impact, the developers state that the model was trained on a supercomputer primarily using nuclear power.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",BigScience RAIL License,[BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
[Midjourney](https://www.midjourney.com/) üìöüñºÔ∏è,Midjourney,"[Midjourney, Inc.](https://www.midjourney.com/)",7/12/2022,Independent Research Lab,United States of America,[Midjourney](https://www.midjourney.com/) is an independent research lab involved in generative AI. Midjourney develops text-to-image models similar to OpenAI‚Äôs DALL-E and Stability AI‚Äôs Stable Diffusion.,"Deep Learning, Computer Vision, Natural Language Processing",Not specified,NaN,"[Midjourney](https://www.midjourney.com/) is a generative artificial intelligence program and service created and hosted by San Francisco-based independent research lab Midjourney, Inc. Midjourney generates images from natural language descriptions (prompts), similar to OpenAI‚Äôs DALL-E and Stability AI‚Äôs Stable Diffusion

Midjourney is currently only accessible through a Discord bot on their official Discord server, by messaging the bot, or by inviting the bot to a third-party server. Users use the /imagine command and type in a prompt to generate images; in response, the bot returns a set of four images. Users may then choose which images they want to upscale.

Beyond the /imagine command, Midjourney offers other commands to send to the Discord bot, like the /blend command (allows the user to blend two images) and the /shorten command (gives the user suggestions on how to make a long prompt shorter).",Not specified,"Text, Image",No,"Disinformation, Algorithmic Discrimination, Environmental Impacts, Technological Unemployment","Midjourney bot has no information about which datasets and methods developers used to train it. On their [website](https://docs.midjourney.com/docs/terms-of-service), the organization details the terms and conditions and community guidelines concerning Midjourney.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[Midjourney Documentation](https://docs.midjourney.com/)
[GPT-NeoX](https://huggingface.co/EleutherAI/gpt-neox-20b) üìö,GPT-NeoX,[EleutherAI](https://www.eleuther.ai),4/14/2022,Non-profit,United States of America,"[EleutherAI](https://www.eleuther.ai/) is a non-profit artificial intelligence research group. The group was formed in a Discord server in July 2020 to organize a replication of GPT-3. In early 2023, it formally incorporated as the EleutherAI Foundation, a non-profit research institute.

According to its [mission statement](https://www.eleuther.ai/about), EleutherAI seeks to (1) advance research on the interpretability and alignment of foundation models, (2) ensure that the ability to study foundation models is not restricted to a handful of companies, and (3) educate people about the capabilities, limitations, and risks associated with these technologies.","Deep Learning, Natural Language Processing",20B,20,"GPT-Neo is a series of large language autoregressive models, from [125M](https://huggingface.co/EleutherAI/gpt-neo-125m) to [20B parameter](https://huggingface.co/EleutherAI/gpt-neox-20b), trained on the Pile, openly available to the public through a permissive license. The GPT-Neo series is EleutherAI's replication of the GPT-3 architecture, while GPT-NeoX is the 20B parameter version. At the time of submission, GPT-NeoX was the largest dense autoregressive model with publicly available weights.

EleutherAI used model parallelism to train GPT-NeoX on a single machine with multiple GPUs, using a combination of techniques such as gradient checkpointing, pipeline parallelism, and mixed precision training to train efficiently. Instead of the learned positional embeddings used in GPT models, the GPT-Neo series uses [rotary embeddings](https://arxiv.org/abs/2104.09864). The final result is GPT-NeoX, which achieves state-of-the-art results on several benchmarks such as LAMBADA, SuperGLUE, and Pile-MNLI.

All GPT-Neo models are open-sourced and licensed under the Apache 2.0 license.",[The Pile](https://pile.eleuther.ai/),Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","According to the authors of [GPT-NeoX paper](https://arxiv.org/abs/2204.06745),  ""_A variety of reasons for the non-release of large language models are given by various groups, but the primary one is the harms that public access to LLMs would purportedly cause._"" In response to this statement, the authors also defend that (1) ""_Providing access to ethics and alignment researchers will prevent harm. The open-source release of this model is motivated by the hope that it will allow researchers who would not otherwise have access to LLMs to use them_"", and (2) ""_Limiting access to governments and corporations will not prevent harm. Perhaps the most curious aspect of the argument that LLMs should not be released is that the people making such arguments are not arguing they should not use LLMs_"".

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.  
  
- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is, text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.  
  
- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.  
  
- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",Apache 2.0 License,[GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)
[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B) üìö,GPT-J,[EleutherAI](https://www.eleuther.ai),6/9/2021,Non-profit,United States of America,"[EleutherAI](https://www.eleuther.ai/) is a non-profit artificial intelligence research group. The group was formed in a Discord server in July 2020 to organize a replication of GPT-3. In early 2023, it formally incorporated as the EleutherAI Foundation, a non-profit research institute.  
  
According to its [mission statement](https://www.eleuther.ai/about), EleutherAI seeks to (1) advance research on the interpretability and alignment of foundation models, (2) ensure that the ability to study foundation models is not restricted to a handful of companies, and (3) educate people about the capabilities, limitations, and risks associated with these technologies.","Deep Learning, Natural Language Processing",6B,6,"[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b) is a 6B parameter autoregressive language model with 28 layers, a model dimension of 4096, and a feedforward dimension of 16384. Like the GPT-Neo series, GPT-J uses [Rotary Position Embeddings](https://huggingface.co/docs/transformers/model_doc/roformer). The model is also trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3.

GPT-J was trained on the Pile, being an addition to the EleutherAI collection of GPT models that can perform various language processing tasks without fine-tuning, such as language translation, code completion, chatting, blog posting, searching information, and much more.
   
GPT-J was trained on the [TPU Research Cloud](https://sites.research.google/trc/about/) and its weights are licensed under version 2.0 of the Apache License.",[The Pile](https://pile.eleuther.ai/),Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","In [GPT-J model card](https://huggingface.co/EleutherAI/gpt-j-6B), the reader can find the potential application risks presented by the GPT-J technology. In it, the authors express that ""_Never depend upon GPT-J to produce factually accurate output. GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results._""

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.  
  
- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is, text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.  
  
- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.  
  
- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",Apache 2.0 License,[GPT-J](https://huggingface.co/EleutherAI/gpt-j-6B)
[Pythia](https://huggingface.co/EleutherAI/pythia-12b) üìö,Pythia,[EleutherAI](https://www.eleuther.ai),4/3/2023,Non-profit,United States of America,"[EleutherAI](https://www.eleuther.ai/) is a non-profit artificial intelligence research group. The group was formed in a Discord server in July 2020 to organize a replication of GPT-3. In early 2023, it formally incorporated as the EleutherAI Foundation, a non-profit research institute.  
  
According to its [mission statement](https://www.eleuther.ai/about), EleutherAI seeks to (1) advance research on the interpretability and alignment of foundation models, (2) ensure that the ability to study foundation models is not restricted to a handful of companies, and (3) educate people about the capabilities, limitations, and risks associated with these technologies.","Deep Learning, Natural Language Processing",12B,12,"[Pythia](https://huggingface.co/EleutherAI/pythia-12b) is a collection of models developed to facilitate interpretability research, trained on sizes 70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two models: one trained on the Pile, and one trained on the Pile after the dataset has been globally deduplicated. All 8 model sizes are trained on the same data, in the same order. EleutherAI also provides 154 intermediate checkpoints per model, hosted on Hugging Face as branches.

The Pythia model suite was deliberately designed to promote scientific research on large language models, especially interpretability research. Despite not centering downstream performance as a design goal, Pythia models [match or exceed](https://huggingface.co/EleutherAI/pythia-12b#evaluations)  the performance of similar and same-sized models, such as those in the OPT and GPT-Neo suites.

All Pythia models are released under the Apache 2.0 License.",[The Pile](https://pile.eleuther.ai/),Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","In [Pythia's model card](https://huggingface.co/EleutherAI/pythia-12b), the reader can find the potential application risks presented by the Pythia technology. In it, the authors express that ""_Never depend upon Pythia to produce factually accurate output. Pythia was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case Pythia may produce socially unacceptable text. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results._""

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.  
  
- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is, text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.  
  
- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.  
  
- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",Apache 2.0 License,[Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)
[Polyglot-Ko](https://huggingface.co/EleutherAI/polyglot-ko-12.8b) üìö,Polyglot-Ko,[EleutherAI](https://www.eleuther.ai),4/3/2023,Non-profit,United States of America,"[EleutherAI](https://www.eleuther.ai/) is a non-profit artificial intelligence research group. The group was formed in a Discord server in July 2020 to organize a replication of GPT-3. In early 2023, it formally incorporated as the EleutherAI Foundation, a non-profit research institute.  
  
According to its [mission statement](https://www.eleuther.ai/about), EleutherAI seeks to (1) advance research on the interpretability and alignment of foundation models, (2) ensure that the ability to study foundation models is not restricted to a handful of companies, and (3) educate people about the capabilities, limitations, and risks associated with these technologies.","Deep Learning, Natural Language Processing",12.8B,12.8,"[Polyglot-Ko](https://huggingface.co/EleutherAI/polyglot-ko-12.8b/tree/main) is a series of large-scale Korean autoregressive language models made by the EleutherAI polyglot team, available in sizes from 1.3B to 12.8B. The model consists of 40 transformer layers with a model dimension of 5120 and a feedforward dimension of 20480. The model dimension is split into 40 heads, each with a dimension of 128. Rotary Position Embedding ([RoPE](https://arxiv.org/abs/2104.09864)) is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 30003. Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token.

Polyglot-Ko-12.8B was trained on 863 GB of Korean language data (1.2TB before processing), a large-scale dataset curated by [TUNiB](https://www.tunib.ai/). The data collection process has abided by South Korean laws, and it will not be released for public use.

All Polyglot-Ko models are released under the Apache 2.0 License.",863 GB of Korean language data curated by [TUNiB](https://tunib.ai/),Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","According to [Polyglot-Ko model card](https://huggingface.co/EleutherAI/polyglot-ko-12.8b#limitations-and-biases), Polyglot-Ko will not always return the most factual or accurate response but the most statistically likely one. In addition, Polyglot may produce socially unacceptable or offensive content. EleutherAI recommends having a human curator or other filtering mechanism to censor sensitive content.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.  
  
- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is, text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.  
  
- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.  
  
- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",Apache 2.0 License,[A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models](https://arxiv.org/abs/2306.02254)
[Code Llama](https://github.com/facebookresearch/codellama) üìö,Code Llama,[Meta AI](https://ai.meta.com/),8/24/2023,Public Company,United States of America,"[Meta AI](https://ai.facebook.com/) is the AI research division of Meta Platforms, Inc., known as Meta] and formerly Facebook, Inc. Meta is an American multinational technology conglomerate based in Menlo Park, California. Meta AI started as Facebook Artificial Intelligence Research (FAIR), announced in September 2013, and initially directed by [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun ""Yann LeCun"").  
  
When it comes to matters of AI Ethics, [Meta AI expresses](https://ai.meta.com/about/) that ""_Our commitment to responsible AI is driven by the belief that everyone should have equitable access to information, services, and opportunities_"". Meta AI also claims to adhere to Meta's core principles: Privacy and security, Fairness and inclusion, Robustness and safety, Transparency and control, Accountability and governance, among other key principles.","Deep Learning, Natural Language Processing",34B,34,"[Code Llama](https://github.com/facebookresearch/codellama) is a family of large language models for code based on Llama 2, trained on a size ranging from 7B to 34B, that has zero-shot instruction following ability for programming tasks. Code Llama was developed by fine-tuning Llama 2 using 500B tokens of publicly available code plus a small portion of natural language datasets related to code. 

Meta AI provides multiple flavors of the Code Llama series, from foundation models (Code Llama) to Python specialists (Code Llama - Python) and even instruction-following models (Code Llama - Instruct). Code Llama and its variants are autoregressive language models using optimized transformer architectures. Code Llama 7B and 13B additionally support infilling text generation.

All Code Llama models are available under the LLaMA 2 Community License Agreement.",500B tokens of publicly available code plus a small portion of natural language datasets related to code,Text,Yes,"Algorithmic Discrimination, Malware Development, Environmental Impacts, Technological Unemployment","According to Llama Code [model card](https://github.com/facebookresearch/codellama/blob/main/MODEL_CARD.md#ethical-considerations-and-limitations),  ""_Code Llama‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any applications of Code Llama, developers should perform safety testing and tuning tailored to their specific applications of the model._""

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üê±‚Äçüë§ Code generation tools can accelerate malware development, enabling malicious actors to launch more sophisticated and effective cyberattacks. These tools may also help lower the intellectual/technique barrier that prevents many people from participating in black hat hacking activities.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",LLaMA 2 Community License Agreement,[Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)
[Github Copilot](https://github.com/features/copilot) üìö,Github Copilot,[Github](https://github.com/),10/29/2021,Subsidiary,United States of America,"[GitHub](https://github.com/) is a platform for hosting source code and other files with version control using [Git](https://git-scm.com/). GitHub was created by Chris Wanstrath, J. Hyett, Tom Preston-Werner, and Scott Chacon in 2008. Currently (2023), the company is a subsidiary of Microsoft, which bought the platform for $7.5 billion in 2018.","Deep Learning, Natural Language Processing",Not specified,NaN,"[GitHub Copilot](https://github.com/features/copilot) is a cloud-based artificial intelligence tool developed by GitHub and OpenAI to assist users of Visual Studio Code, Visual Studio, Neovim, and JetBrains integrated development environments (IDEs) by autocompleting code. 

GitHub Copilot is powered by the OpenAI Codex, being a modified, production version of the Generative Pre-trained Transformer 3 (GPT-3). When provided with a programming problem in natural language, Copilot generates a solution in code. It can also describe input code in English, autocomplete code sequences, and translate code between programming languages.",Not specified,Text,No,"Algorithmic Discrimination, Malware Development, Environmental Impacts, Technological Unemployment","GitHub mentions on its website that given that public sources are predominantly in English, Github Copilot is likely to work less well in scenarios where the natural language prompts provided by the developer are not in English and/or are grammatically incorrect. GitHub also reports that Copilot‚Äôs autocomplete feature is accurate roughly half of the time. Other possible risks can be inferred by the analysis of the foundational model that powers the application ([Codex](https://arxiv.org/abs/2107.03374)).

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üê±‚Äçüë§ Code generation tools can accelerate malware development, enabling malicious actors to launch more sophisticated and effective cyberattacks. These tools may also help lower the intellectual/technique barrier that prevents many people from participating in black hat hacking activities.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[Your AI pair programmer](https://github.com/features/copilot)
[Parti](https://github.com/google-research/parti) üìöüñºÔ∏è,Parti,[Google Research](https://research.google/),6/22/2022,Subsidiary,United States of America,"[Google Research](https://research.google/) is a research division of Google that focuses on advancing computer science, machine learning, artificial intelligence, and other related fields. Meanwhile, Google is a subsidiary of Alphabet Inc., a publicly traded company with multiple classes of shareholders.  
  
Google Research is responsible for developing new technologies and products that can be used by Google and its users. The division has several research areas, including natural language processing, computer vision, robotics, and more. Google Research, like other Google-related organizations, abides by [Google's AI Principles](https://ai.google/responsibility/principles/).","Deep Learning, Computer Vision, Natural Language Processing",20B,20,"Pathways Autoregressive Text-to-Image model ([Parti](https://github.com/google-research/parti)) is a series of autoregressive text-to-image generation models, from 350M to 20B parameters,  that achieves high-fidelity photorealistic image generation. Unlike Google‚Äôs [Imagen](https://imagen.research.google/), a diffusion model, Parti is an autoregressive model.  
  
Parti treats text-to-image generation as a sequence-to-sequence modeling problem, analogous to machine translation ‚Äì which allows it to benefit from advances in large language models, especially capabilities that are unlocked by scaling data and model sizes. In this case, the target outputs are sequences of image tokens instead of text tokens in another language. Parti uses an image tokenizer, [ViT-VQGAN](https://ai.googleblog.com/2022/05/vector-quantized-image-modeling-with.html), to encode images as sequences of discrete tokens, and then reconstructs the image token sequences as images.

Parti models, code, and data are not available to the public.","[LAION-400M dataset](https://huggingface.co/datasets/laion/laion400m), [ALIGN training data](https://arxiv.org/abs/2102.05918), and the [JFT-4B dataset](https://paperswithcode.com/paper/scaling-vision-transformers)","Text, Image",Yes,"Disinformation, Algorithmic Discrimination, Environmental Impacts, Technological Unemployment","According to [the authors](https://arxiv.org/abs/2206.10789), there are broader issues to consider with large-scale models for text-to-image generation, like visual (mis)communication and deepfakes, problems concerning stereotyping, pornography, violence, among other problems.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[Scaling Autoregressive Models for Content-Rich Text-to-Image Generation](https://arxiv.org/abs/2206.10789)
 [Imagen](https://imagen.research.google/) üìöüñºÔ∏è,Imagen,[Google Research](https://research.google/),5/23/2022,Subsidiary,United States of America,"[Google Research](https://research.google/) is a research division of Google that focuses on advancing computer science, machine learning, artificial intelligence, and other related fields. Meanwhile, Google is a subsidiary of Alphabet Inc., a publicly traded company with multiple classes of shareholders.  
  
Google Research is responsible for developing new technologies and products that can be used by Google and its users. The division has several research areas, including natural language processing, computer vision, robotics, and more. Google Research, like other Google-related organizations, abides by [Google's AI Principles](https://ai.google/responsibility/principles/).","Deep Learning, Computer Vision, Natural Language Processing",14B,14,"[Imagen](https://imagen.research.google/) is a text-to-image diffusion model that builds on the language understanding capabilities of large transformer language models and the capacities of diffusion models to promote high-fidelity image generation.

Imagen uses a large frozen [T5-XXL](https://huggingface.co/google/t5-efficient-xxl) encoder to encode the input text into embeddings. Then, a conditional diffusion model maps the text embedding into a 64√ó64 image. Imagen further utilizes text-conditional super-resolution diffusion models to upsample the 64√ó64 image to 256√ó256, and later to 1024√ó1024.

Imagen is a text-to-image model that has not been released for public use due to concerns regarding responsible open-sourcing of code and demos.",860 million text-image pairs from Google's internal datasets and the [Laion](https://huggingface.co/datasets/laion/laion400m) dataset,"Text, Image",Yes,"Disinformation, Algorithmic Discrimination, Environmental Impacts, Technological Unemployment","According to Google Research, given that Imagen relies on text encoders trained on uncurated web-scale data, this model also inherits social biases and limitations characteristic of large language models. Meanwhile, the use of [LAION-400M](https://laion.ai/blog/laion-400-open-dataset/) for Imagen training, which is known for containing inappropriate content, including pornographic, racist images and social stereotypes, also highlights the ethical limitations of this technology.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/abs/2205.11487)
[PaLM](https://arxiv.org/abs/2204.02311) üìö,PaLM,[Google Research](https://research.google/),10/5/2022,Subsidiary,United States of America,"[Google Research](https://research.google/) is a research division of Google that focuses on advancing computer science, machine learning, artificial intelligence, and other related fields. Meanwhile, Google is a subsidiary of Alphabet Inc., a publicly traded company with multiple classes of shareholders.  
  
Google Research is responsible for developing new technologies and products that can be used by Google and its users. The division has several research areas, including natural language processing, computer vision, robotics, and more. Google Research, like other Google-related organizations, abides by [Google's AI Principles](https://ai.google/responsibility/principles/).","Deep Learning, Natural Language Processing",540B,540,"The Pathways Language Model, or [PaLM](https://arxiv.org/abs/2204.02311), consists of a series of large language models, with sizes ranging from 8 billion, 62 billion, to 540 billion parameters. The development of PaLM was made possible through the utilization of Pathways, a machine learning system introduced in the [Pathways](https://arxiv.org/abs/2203.01253) paper  Pathways is designed to facilitate the highly efficient training of large neural networks, leveraging the power of thousands of accelerator chips.

PaLM's training data draws from diverse sources, including English and multilingual datasets that encompass high-quality web documents, books, Wikipedia, conversations, and GitHub code. Among PaLM's capabilities, one can cite language understanding, generation, reasoning, and code-related tasks.

Currently (2023), PaLM is not released to the public.",780 billion tokens that represent a wide range of natural language use cases,Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Malware Development, Environmental Impacts, Technological Unemployment, Intelectual Fraud","According to Google Research, PaLM can generate text that perpetuates or exacerbates social stereotypes and disparities reflected in its training data. Hence, any real-world use of PaLM for downstream tasks should perform further contextualized fairness evaluations to assess the potential harms and introduce appropriate mitigation and protections. Also, the developers mention that there is potential for malicious use of such PaLM-like models with proficiency in mimicking human language behavior, which may be used in use cases such as misinformation campaigns.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üê±‚Äçüë§ Code generation tools can accelerate malware development, enabling malicious actors to launch more sophisticated and effective cyberattacks. These tools may also help lower the intellectual/technique barrier that prevents many people from participating in black hat hacking activities.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.

- üë®‚Äçüéì Generative models can automate the process of academic writing and intellectual creation. Such systems can impact how educational institutions function and how intellectual property laws are designed and implemented.",Proprietary License,[PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)
[PaLM 2](https://ai.google/discover/palm2) üìö,PaLM 2,[Google AI](https://ai.google/),5/1/2023,Subsidiary,United States of America,"[Google AI](https://ai.google/) is a research division at Google that focuses on developing artificial intelligence. Meanwhile, Google is a subsidiary of Alphabet Inc., a publicly traded company with multiple classes of shareholders. Google AI offers a range of machine learning products, solutions, and services that are powered by its research and technology. 

According to Google AI, ""_While we are optimistic about the potential of AI, we recognize that advanced technologies can raise important challenges that must be addressed, thoughtfully, and affirmatively.  These AI Principles describe our commitment to developing technology responsibly and work to establish specific application areas we will not pursue._"" More on the principles that guide Google AI can be found on their [website](https://ai.google/responsibility/principles/).","Deep Learning, Natural Language Processing",Not specified,NaN,"[PaLM 2](https://ai.google/discover/palm2) is a state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor ([PaLM](https://arxiv.org/abs/2204.02311)). The largest model in the PaLM 2 series, PaLM 2-L, is significantly smaller than the largest PaLM model but uses more training computing. The actual sizes (parameter count) of the PaLM 2 series are unknown.

According to Google AI, results show that PaLM 2 models significantly outperform PaLM on a variety of tasks, including natural language generation, translation, and reasoning. These results suggest that model scaling is not the only way to improve performance. Instead, performance can be unlocked by meticulous data selection and efficient architecture/objectives.

PaLM 2 is the foundation that powers other state-of-the-art models, like [Sec-PaLM](https://cloud.google.com/blog/products/identity-security/rsa-google-cloud-security-ai-workbench-generative-ai) and [Bard](https://bard.google.com/).","A diverse set of sources containing web documents, books, code, mathematics, and conversational data",Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Malware Development, Environmental Impacts, Technological Unemployment, Intelectual Fraud","According to Google AI, PaLM 2 and all of its downstream applications have been developed under [Google‚Äôs AI Principles](https://ai.google/responsibility/principles/), which outline objectives and applications they will not pursue, e.g., technologies that cause or are likely to cause overall harm, technologies whose principal purpose or implementation is to cause or directly facilitate injury to people, technologies that gather or use information for surveillance violating internationally accepted norm, and technologies whose purpose contravenes widely accepted principles of international law and human rights.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üê±‚Äçüë§ Code generation tools can accelerate malware development, enabling malicious actors to launch more sophisticated and effective cyberattacks. These tools may also help lower the intellectual/technique barrier that prevents many people from participating in black hat hacking activities.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.

- üë®‚Äçüéì Generative models can automate the process of academic writing and intellectual creation. Such systems can impact how educational institutions function and how intellectual property laws are designed and implemented.",Proprietary License,[PaLM 2 Technical Report](https://ai.google/static/documents/palm2techreport.pdf)
[AudioLM](https://arxiv.org/abs/2209.03143) üì¢,AudioLM,[Google Research](https://research.google/),7/26/2023,Subsidiary,United States of America,"[Google Research](https://research.google/) is a research division of Google that focuses on advancing computer science, machine learning, artificial intelligence, and other related fields. Meanwhile, Google is a subsidiary of Alphabet Inc., a publicly traded company with multiple classes of shareholders.  
  
Google Research is responsible for developing new technologies and products that can be used by Google and its users. The division has several research areas, including natural language processing, computer vision, robotics, and more. Google Research, like other Google-related organizations, abides by [Google's AI Principles](https://ai.google/responsibility/principles/).","Deep Learning, Natural Language Processing, Speech Recognition",Not specified,NaN,"[AudioLM](https://arxiv.org/abs/2209.03143) is a framework for a high-quality audio generation with long-term consistency that can map the input audio to a sequence of discrete tokens and cast audio generation as a language modeling task. According to Google Research, models trained via this framework learn to generate natural and coherent continuations given short audio prompts. Furthermore, this approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.

Models trained via the AudioLM are comprised of different components: [SoundStream](https://arxiv.org/abs/2107.03312) (i.e., neural audio codec that can efficiently compress speech), [w2v-BERT](https://arxiv.org/abs/2108.06209), the k-means quantizer for w2v-BERT embeddings, and a decoder-only Transformer. Models were trained on the unlab-60k train split of [Libri-Light](https://github.com/facebookresearch/libri-light), consisting of 60k hours of English speech.

AudioLM models are not available to the public, but demos of its capabilities are exposed in the following [URL](https://google-research.github.io/seanet/audiolm/examples/).","The train split of [unlab-60k](https://github.com/facebookresearch/libri-light), consisting of 60k hours of English speech",Audio,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts, Technological Unemployment","According to Google Research,  there are several risks associated with AudioLM. For example, AudioLM inherits all concerns about language models for text, such as reflecting the societal biases in the underlying data. Furthermore, the generated speech continuations might not be consistent with the prompt in terms of accent and dialect for underrepresented groups in the training data. Also, the ability to continue short speech segments while maintaining speaker identity and prosody can potentially lead to malicious use cases such as spoofing biometric identification or impersonating a specific speaker.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[AudioLM: a Language Modeling Approach to Audio Generation](https://arxiv.org/abs/2209.03143)
[Muse](https://muse-model.github.io/) üìöüñºÔ∏è,Muse,[Google Research](https://research.google/),1/2/2023,Subsidiary,United States of America,"[Google Research](https://research.google/) is a research division of Google that focuses on advancing computer science, machine learning, artificial intelligence, and other related fields. Meanwhile, Google is a subsidiary of Alphabet Inc., a publicly traded company with multiple classes of shareholders.  
  
Google Research is responsible for developing new technologies and products that can be used by Google and its users. The division has several research areas, including natural language processing, computer vision, robotics, and more. Google Research, like other Google-related organizations, abides by [Google's AI Principles](https://ai.google/responsibility/principles/).","Deep Learning, Computer Vision, Natural Language Processing",3B,3,"[Muse](https://muse-model.github.io/) is a text-to-image Transformer model trained on a masked modeling task, i.e., given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens, similar to what language models trained via MLM (masked language modeling), like BERT and RoBERTa do. Muse models achieve SOTA on benchmarks like CC3M and COCO, being also able to directly enable several image editing applications without the need to fine-tune or invert the model.

Muse models come in a size range from 632M parameters to 3B parameters. Each model consists of several sub-models: (1) a pair of VQGAN tokenizer models that can encode an input image to a sequence of discrete tokens as well as decode a token sequence back to an image; (2) a base masked image model conditioned on the unmasked tokens and a T5XXL; and (3) a ""_superres_"" transformer model which translates (unmasked) low-res tokens into high-res tokens (also conditioned on the unmasked tokens and a T5XXL).

Muse models are not open to the public, but a demo can be found in this [URL](https://muse-model.github.io/).",[Imagen](https://imagen.research.google/) dataset consisting of 460M text-image pairs,"Text, Image",Yes,"Disinformation, Algorithmic Discrimination, Environmental Impacts, Technological Unemployment","According to Google Research, generative models have several applications with varied potential to impact human society. For example, generative models can be leveraged for misinformation, harassment, and various social and cultural biases. Due to these considerations, Google Research opted not to release code or a public demo at the time of Muse's release.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.

- üë∑ A significant portion of the workforce today still performs many tasks that can be automated using generative models and low-level AI systems. In some industries, such technologies might provoke considerable labor displacement.",Proprietary License,[Muse: Text-To-Image Generation via Masked Generative Transformers](https://arxiv.org/abs/2301.00704)
[LaMDA](https://arxiv.org/abs/2201.08239) üìö,LaMDA,[Google](https://about.google/),2/10/2022,Subsidiary,United States of America,"[Google](https://about.google/) is an American multinational technology company focusing on artificial intelligence, online advertising, search engine technology, cloud computing, computer software, quantum computing, e-commerce, and consumer electronics.  

Google is also a subsidiary of Alphabet Inc., a publicly traded company with multiple classes of shareholders. Google abides by [Google's AI Principles](https://ai.google/responsibility/principles/).","Deep Learning, Natural Language Processing",137B,137,"[LaMDA](https://arxiv.org/abs/2201.08239) is a family of Transformer-based neural language models specialized for dialog. These models‚Äô sizes range from 2B to 137B parameters, and they are pre-trained on a dataset containing 1.56T words of public dialog data and web text.

LaMDA has been pre-trained in causal language modeling and can used as a general generative language model before fine-tuning. To improve general performance, Google uses a series of classifiers and information retrieval systems to enhance the generative capabilities of LaMDA, where rejection sampling techniques filter the completions in terms of quality, safety, and groundedness.
  
LaMDA is not available to the public.",2.81T tokens from public dialog data and other public web documents,Text,Yes,"Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts","According to Google, many fundamental challenges to developing a high-quality dialog model capable of performing well in real-world applications still exist. For example, it is now increasingly well-understood that large language models trained on unlabeled datasets will learn to imitate patterns and biases inherent in their training sets. Besides, Google mentions the vulnerabilities of LaMDA to adversarial attacks and the risk that deliberate misuse of these technologies might deceive or manipulate people, inadvertently or with malicious intent.

- ü§• Generative AI models, like LLMs used for text generation/conversation or GANs for image generation, can produce content that can be mistaken for truth but is, in fact, misleading or entirely false, given the model's tendency to output hallucinations. Such models can generate deceptive visuals, human-like textual content, music, or combined media that might seem genuine at first glance.

- ü§¨ Machine learning systems can inherit social and historical stereotypes from the data used to train them. Given these biases, models can be prone to produce toxic content, that is,  text, images, videos, or comments, that is harmful, offensive, or detrimental to individuals, groups, or communities. Also, models that automate decision-making can have biases against certain groups, affecting people based on sensitive attributes in an unjust manner.

- üé£ Generative models that can produce human-like content can be used by malicious actors to intentionally cause harm through social engineering techniques like phishing and large-scale fraud. Also, anthropomorphizing AI models can lead to unrealistic expectations and a lack of understanding of the limitations and capabilities of the technology.

- üè≠ The development of large machine learning models can have significant environmental impacts due to the high energy consumption required for their training. Given the current state of energy mixes in most countries, high energy consumption can contribute to the injection of large amounts of CO2 equivalents into the atmosphere, further pushing the planetary boundaries tied to our current climate crisis.",Proprietary License,[LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239)