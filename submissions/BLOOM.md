# BLOOM

## Model Details

- Name: [BLOOM](https://huggingface.co/bigscience/bloom) ðŸ“š
- Model Size: 176B
- Dataset: [ROOTS corpus](https://arxiv.org/abs/2303.03915), a dataset comprising hundreds of sources in 46 natural and 13 programming languages (366B tokens)
- Input/Output Format: Text
- Research Field: Natural Language Processing
- Contains an Impact Assessment: Yes
- Associated Risks: Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts
- Date of Publication: 11/9/2022
- Organization: [BigScience](https://bigscience.huggingface.co/) (Academic/Research Institution)
- Country/Origin: France
- License: BigScience RAIL License
- Publication: [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)

## Description

BigScience Large Open-science Open-access Multilingual Language Model ([BLOOM](https://huggingface.co/bigscience/bloom)) is a transformer-based large language model created by over 1,000 AI researchers. BLOOM was trained on around 366 billion tokens from March through July 2022, and it was one of the first open alternatives to large language models like GPT-3.

BLOOM uses a decoder-only transformer model architecture modified from Megatron-LM GPT-2. It can output coherent text in 46 languages and 13 programming languages that are hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for.

BLOOM was trained in six sizes ranging from 560 million to 176 billion parameters. All models are [publicly available](https://huggingface.co/bigscience) and released under the Responsible AI License.

## Organization

[BigScience](https://bigscience.huggingface.co/) is an open and collaborative workshop around studying and creating very large language models, gathering more than 1000 researchers worldwide.

According to their [homepage](https://bigscience.notion.site/Introduction-5facbf41a16848d198bda853485e23a0), the BigScience project is inspired by existing partnership projects in other fields, such as [CERN](https://home.web.cern.ch/), [LIGO]( https://www.ligo.caltech.edu/), and [ITER](https://www.iter.org/), in which research collaborations are open, facilitating large-scale results.
