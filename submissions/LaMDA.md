# LaMDA

## Model Details

- Name: [LaMDA](https://arxiv.org/abs/2201.08239) ðŸ“š
- Model Size: 137B
- Dataset: 2.81T tokens from public dialog data and other public web documents
- Input/Output Format: Text
- Research Field: Deep Learning, Natural Language Processing
- Contains an Impact Assessment: Yes
- Associated Risks: Disinformation, Algorithmic Discrimination, Social Engineering, Environmental Impacts
- Date of Publication: 2/10/2022
- Organization: [Google](https://about.google/) (Subsidiary)
- Country/Origin: United States of America
- License: Proprietary License
- Publication: [LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239)

## Description

[LaMDA](https://arxiv.org/abs/2201.08239) is a family of Transformer-based neural language models specialized for dialog. These modelsâ€™ sizes range from 2B to 137B parameters, and they are pre-trained on a dataset containing 1.56T words of public dialog data and web text.

LaMDA has been pre-trained in causal language modeling and can used as a general generative language model before fine-tuning. To improve general performance, Google uses a series of classifiers and information retrieval systems to enhance the generative capabilities of LaMDA, where rejection sampling techniques filter the completions in terms of quality, safety, and groundedness.
  
LaMDA is not available to the public.

## Organization

[Google](https://about.google/) is an American multinational technology company focusing on artificial intelligence, online advertising, search engine technology, cloud computing, computer software, quantum computing, e-commerce, and consumer electronics.  

Google is also a subsidiary of Alphabet Inc., a publicly traded company with multiple classes of shareholders. Google abides by [Google's AI Principles](https://ai.google/responsibility/principles/).
